{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e01fb439",
   "metadata": {},
   "source": [
    "# Data and Analysis Plan: Secure. Contain. Protect.\n",
    "\n",
    "- Lee Fenuccio\n",
    "- Rudra Sett\n",
    "- Connor Brown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea622cb",
   "metadata": {},
   "source": [
    "## Project Goal:\n",
    "This project uses data scraped from the SCP wiki to analyze similarities and differences between SCPs. Using these analyzes, we can build classifiers to predict attributes of new SCPs, and recommend different SCPs based on someone's interests. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e72ae71",
   "metadata": {},
   "source": [
    "## Data \n",
    "\n",
    "### Overview \n",
    "We will scrape data off of the [SCP Wiki](https://scp-wiki.wikidot.com/) to obtain data for each SCP. The SCP Wiki has a unique page for each SCP object that contains information about it. Our pipeline will extract specific attributes of an SCP and put them into a dataframe to be used for analysis. Below is a sample of a page for an SCP off of the SCP wiki, [SCP-4141](https://scp-wiki.wikidot.com/scp-4141). \n",
    "![Wiki](Wiki_Screenshot.jpg)\n",
    "\n",
    "Some easily visible attributes of the SCP are its number (4141), Class (Safe) and Rating (+102). Based on a more in depth knowledge about SCPs, there are other tropes that we searched the scraped text for, such as if the object had any sub-objects or mentions of D-Class personnel. Then we will use some natural language processing to gather more information about the style of how the SCP was written, such as sentiment analysis. \n",
    "\n",
    "### Pipeline Overview\n",
    "\n",
    "We will accomplish this task with the following functions:\n",
    "\n",
    "#### Webscraping\n",
    "- 'rand_scp_num()'\n",
    "    - gets a random number in the form of the scp wiki url\n",
    "- 'get_scp_soup()'\n",
    "    - gets soup object for a given scp number\n",
    "- 'get_scp_maintext()'\n",
    "    - gets the main text from the scraped scp\n",
    "- 'get_rating()'\n",
    "    - gets the positive and overall ratings from the scraped scp\n",
    "- 'get_subobjects()'\n",
    "    - get a list of subobjects 1-9 and A-Z that exist in scraped scp\n",
    "- 'get_tags()'\n",
    "    - gets the tags for the scraped scp\n",
    "    \n",
    "#### Formatting Data\n",
    "- 'get_dict()'\n",
    "    - gets the dictionary with attributes for a certain scp\n",
    "- 'get_random_dataframe()'\n",
    "    - creates a dataframe from a specified number of random scps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffe627f",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "#### Webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86db3ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2942e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bb3ebaf",
   "metadata": {},
   "source": [
    "#### Formatting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e500d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fba453",
   "metadata": {},
   "source": [
    "## Analysis Plan\n",
    "We are looking to analyze the data we get from the SCPs and use it to make decisions about what attributes a new SCP should have, and an algorithmn to recommend SCPs to read based off of someone's preferences. \n",
    "\n",
    "One algorthimn could be to use a Random Forest to look through a new SCP and decide what its class should be. Since the entries all have their class directly written in their text, this will have to be removed from the attribute output so it isn't already known about an SCP. This will involve creating a dataframe of many SCPs and their attributes, and converting all attributes to numerical values. This will be easy for things such as True/False, but more difficult for things such as page tags, since there are so many. Then this data will be implemented into a Random Forest classifier. We can use cross-validation to check its accuracy.\n",
    "\n",
    "Another data analysis we are interested in doing but haven't learned in class yet is using the attributes of one point and finding the n most similar points to it. This could be done very similarly to the K-Nearest Neighbor classifier, but instead of using the nearest neighbors to estimate a category, we could save what the n nearest neighbors are. This would be similar to Netflix's 'More like this' recommmendations. We give the algorithmn an SCP or a few, and it returns several SCPs that have similar attributes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
